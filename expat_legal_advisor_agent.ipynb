{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "073ef9da",
      "metadata": {
        "id": "073ef9da"
      },
      "source": [
        "# Expat Legal Aid Advisor ‚Äî Complete End-to-End Notebook (Enhanced)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8ac9421e",
      "metadata": {
        "id": "8ac9421e"
      },
      "source": [
        "This notebook sets up the full multi-agent system, runs tests with coverage, and launches a Gradio UI. It also includes **smoke tests**, **PDF/DOCX parsing checks**, and a **translation-fallback** path."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cb854521",
      "metadata": {
        "id": "cb854521"
      },
      "source": [
        "## 1) Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "605dcd24",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "605dcd24",
        "outputId": "ac568a2c-cd90-499e-a57a-d049b4522d1e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing requirements.txt\n"
          ]
        }
      ],
      "source": [
        "%%writefile requirements.txt\n",
        "gradio\n",
        "flask\n",
        "pytest\n",
        "coverage\n",
        "cryptography\n",
        "requests\n",
        "google-generativeai\n",
        "Flask-HTTPAuth\n",
        "Flask-Limiter\n",
        "gunicorn\n",
        "PyPDF2\n",
        "reportlab\n",
        "python-docx\n",
        "langdetect\n",
        "jedi\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "weiR2pKCqXKY",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "weiR2pKCqXKY",
        "outputId": "ff7494af-4673-4d31-c89e-8d3d9355ba71"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gradio in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 1)) (5.50.0)\n",
            "Requirement already satisfied: flask in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 2)) (3.1.2)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 3)) (8.4.2)\n",
            "Collecting coverage (from -r requirements.txt (line 4))\n",
            "  Downloading coverage-7.12.0-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (9.1 kB)\n",
            "Requirement already satisfied: cryptography in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 5)) (43.0.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 6)) (2.32.4)\n",
            "Requirement already satisfied: google-generativeai in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 7)) (0.8.5)\n",
            "Collecting Flask-HTTPAuth (from -r requirements.txt (line 8))\n",
            "  Downloading Flask_HTTPAuth-4.8.0-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting Flask-Limiter (from -r requirements.txt (line 9))\n",
            "  Downloading flask_limiter-4.0.0-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting gunicorn (from -r requirements.txt (line 10))\n",
            "  Downloading gunicorn-23.0.0-py3-none-any.whl.metadata (4.4 kB)\n",
            "Collecting PyPDF2 (from -r requirements.txt (line 11))\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting reportlab (from -r requirements.txt (line 12))\n",
            "  Downloading reportlab-4.4.5-py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting python-docx (from -r requirements.txt (line 13))\n",
            "  Downloading python_docx-1.2.0-py3-none-any.whl.metadata (2.0 kB)\n",
            "Collecting langdetect (from -r requirements.txt (line 14))\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting jedi (from -r requirements.txt (line 15))\n",
            "  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.12/dist-packages (from gradio->-r requirements.txt (line 1)) (24.1.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio->-r requirements.txt (line 1)) (4.11.0)\n",
            "Requirement already satisfied: brotli>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from gradio->-r requirements.txt (line 1)) (1.2.0)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.12/dist-packages (from gradio->-r requirements.txt (line 1)) (0.118.3)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.12/dist-packages (from gradio->-r requirements.txt (line 1)) (1.0.0)\n",
            "Requirement already satisfied: gradio-client==1.14.0 in /usr/local/lib/python3.12/dist-packages (from gradio->-r requirements.txt (line 1)) (1.14.0)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.12/dist-packages (from gradio->-r requirements.txt (line 1)) (0.1.2)\n",
            "Requirement already satisfied: httpx<1.0,>=0.24.1 in /usr/local/lib/python3.12/dist-packages (from gradio->-r requirements.txt (line 1)) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub<2.0,>=0.33.5 in /usr/local/lib/python3.12/dist-packages (from gradio->-r requirements.txt (line 1)) (0.36.0)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.12/dist-packages (from gradio->-r requirements.txt (line 1)) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio->-r requirements.txt (line 1)) (3.0.3)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.12/dist-packages (from gradio->-r requirements.txt (line 1)) (2.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio->-r requirements.txt (line 1)) (3.11.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from gradio->-r requirements.txt (line 1)) (25.0)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.12/dist-packages (from gradio->-r requirements.txt (line 1)) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.12/dist-packages (from gradio->-r requirements.txt (line 1)) (11.3.0)\n",
            "Requirement already satisfied: pydantic<=2.12.3,>=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio->-r requirements.txt (line 1)) (2.12.3)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.12/dist-packages (from gradio->-r requirements.txt (line 1)) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.12/dist-packages (from gradio->-r requirements.txt (line 1)) (0.0.20)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.12/dist-packages (from gradio->-r requirements.txt (line 1)) (6.0.3)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.12/dist-packages (from gradio->-r requirements.txt (line 1)) (0.14.6)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.12/dist-packages (from gradio->-r requirements.txt (line 1)) (0.1.7)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio->-r requirements.txt (line 1)) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.12/dist-packages (from gradio->-r requirements.txt (line 1)) (0.48.0)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from gradio->-r requirements.txt (line 1)) (0.13.3)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.12/dist-packages (from gradio->-r requirements.txt (line 1)) (0.20.0)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.12/dist-packages (from gradio->-r requirements.txt (line 1)) (4.15.0)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.12/dist-packages (from gradio->-r requirements.txt (line 1)) (0.38.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from gradio-client==1.14.0->gradio->-r requirements.txt (line 1)) (2025.3.0)\n",
            "Requirement already satisfied: websockets<16.0,>=13.0 in /usr/local/lib/python3.12/dist-packages (from gradio-client==1.14.0->gradio->-r requirements.txt (line 1)) (15.0.1)\n",
            "Requirement already satisfied: blinker>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from flask->-r requirements.txt (line 2)) (1.9.0)\n",
            "Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.12/dist-packages (from flask->-r requirements.txt (line 2)) (8.3.1)\n",
            "Requirement already satisfied: itsdangerous>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from flask->-r requirements.txt (line 2)) (2.2.0)\n",
            "Requirement already satisfied: werkzeug>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from flask->-r requirements.txt (line 2)) (3.1.3)\n",
            "Requirement already satisfied: iniconfig>=1 in /usr/local/lib/python3.12/dist-packages (from pytest->-r requirements.txt (line 3)) (2.3.0)\n",
            "Requirement already satisfied: pluggy<2,>=1.5 in /usr/local/lib/python3.12/dist-packages (from pytest->-r requirements.txt (line 3)) (1.6.0)\n",
            "Requirement already satisfied: pygments>=2.7.2 in /usr/local/lib/python3.12/dist-packages (from pytest->-r requirements.txt (line 3)) (2.19.2)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.12/dist-packages (from cryptography->-r requirements.txt (line 5)) (2.0.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->-r requirements.txt (line 6)) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->-r requirements.txt (line 6)) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->-r requirements.txt (line 6)) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->-r requirements.txt (line 6)) (2025.11.12)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in /usr/local/lib/python3.12/dist-packages (from google-generativeai->-r requirements.txt (line 7)) (0.6.15)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.12/dist-packages (from google-generativeai->-r requirements.txt (line 7)) (2.28.1)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.12/dist-packages (from google-generativeai->-r requirements.txt (line 7)) (2.187.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.12/dist-packages (from google-generativeai->-r requirements.txt (line 7)) (2.43.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from google-generativeai->-r requirements.txt (line 7)) (5.29.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from google-generativeai->-r requirements.txt (line 7)) (4.67.1)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.12/dist-packages (from google-ai-generativelanguage==0.6.15->google-generativeai->-r requirements.txt (line 7)) (1.26.1)\n",
            "Collecting limits>=3.13 (from Flask-Limiter->-r requirements.txt (line 9))\n",
            "  Downloading limits-5.6.0-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting ordered-set<5,>4 (from Flask-Limiter->-r requirements.txt (line 9))\n",
            "  Downloading ordered_set-4.1.0-py3-none-any.whl.metadata (5.3 kB)\n",
            "Requirement already satisfied: rich<15,>=12 in /usr/local/lib/python3.12/dist-packages (from Flask-Limiter->-r requirements.txt (line 9)) (13.9.4)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from python-docx->-r requirements.txt (line 13)) (6.0.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from langdetect->-r requirements.txt (line 14)) (1.17.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.12/dist-packages (from jedi->-r requirements.txt (line 15)) (0.8.5)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0,>=3.0->gradio->-r requirements.txt (line 1)) (1.3.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.12->cryptography->-r requirements.txt (line 5)) (2.23)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core->google-generativeai->-r requirements.txt (line 7)) (1.72.0)\n",
            "Requirement already satisfied: cachetools<7.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth>=2.15.0->google-generativeai->-r requirements.txt (line 7)) (6.2.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth>=2.15.0->google-generativeai->-r requirements.txt (line 7)) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth>=2.15.0->google-generativeai->-r requirements.txt (line 7)) (4.9.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0,>=0.24.1->gradio->-r requirements.txt (line 1)) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0,>=0.24.1->gradio->-r requirements.txt (line 1)) (0.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.33.5->gradio->-r requirements.txt (line 1)) (3.20.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.33.5->gradio->-r requirements.txt (line 1)) (1.2.0)\n",
            "Collecting deprecated>=1.2 (from limits>=3.13->Flask-Limiter->-r requirements.txt (line 9))\n",
            "  Downloading deprecated-1.3.1-py2.py3-none-any.whl.metadata (5.9 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio->-r requirements.txt (line 1)) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio->-r requirements.txt (line 1)) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio->-r requirements.txt (line 1)) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<=2.12.3,>=2.0->gradio->-r requirements.txt (line 1)) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<=2.12.3,>=2.0->gradio->-r requirements.txt (line 1)) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<=2.12.3,>=2.0->gradio->-r requirements.txt (line 1)) (0.4.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich<15,>=12->Flask-Limiter->-r requirements.txt (line 9)) (4.0.0)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio->-r requirements.txt (line 1)) (1.5.4)\n",
            "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client->google-generativeai->-r requirements.txt (line 7)) (0.31.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client->google-generativeai->-r requirements.txt (line 7)) (0.2.1)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client->google-generativeai->-r requirements.txt (line 7)) (4.2.0)\n",
            "Requirement already satisfied: wrapt<3,>=1.10 in /usr/local/lib/python3.12/dist-packages (from deprecated>=1.2->limits>=3.13->Flask-Limiter->-r requirements.txt (line 9)) (2.0.1)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai->-r requirements.txt (line 7)) (1.76.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai->-r requirements.txt (line 7)) (1.71.2)\n",
            "Requirement already satisfied: pyparsing<4,>=3.0.4 in /usr/local/lib/python3.12/dist-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google-generativeai->-r requirements.txt (line 7)) (3.2.5)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich<15,>=12->Flask-Limiter->-r requirements.txt (line 9)) (0.1.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai->-r requirements.txt (line 7)) (0.6.1)\n",
            "Downloading coverage-7.12.0-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (252 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m252.3/252.3 kB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Flask_HTTPAuth-4.8.0-py3-none-any.whl (7.0 kB)\n",
            "Downloading flask_limiter-4.0.0-py3-none-any.whl (29 kB)\n",
            "Downloading gunicorn-23.0.0-py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m85.0/85.0 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading reportlab-4.4.5-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m58.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_docx-1.2.0-py3-none-any.whl (252 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m253.0/253.0 kB\u001b[0m \u001b[31m25.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m87.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading limits-5.6.0-py3-none-any.whl (60 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ordered_set-4.1.0-py3-none-any.whl (7.6 kB)\n",
            "Downloading deprecated-1.3.1-py2.py3-none-any.whl (11 kB)\n",
            "Building wheels for collected packages: langdetect\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993223 sha256=9698de8d1577c2915aa4e06f104a8eba9898d224e3ea0cb1a972312b61785eb3\n",
            "  Stored in directory: /root/.cache/pip/wheels/c1/67/88/e844b5b022812e15a52e4eaa38a1e709e99f06f6639d7e3ba7\n",
            "Successfully built langdetect\n",
            "Installing collected packages: reportlab, python-docx, PyPDF2, ordered-set, langdetect, jedi, gunicorn, deprecated, coverage, limits, Flask-Limiter, Flask-HTTPAuth\n",
            "Successfully installed Flask-HTTPAuth-4.8.0 Flask-Limiter-4.0.0 PyPDF2-3.0.1 coverage-7.12.0 deprecated-1.3.1 gunicorn-23.0.0 jedi-0.19.2 langdetect-1.0.9 limits-5.6.0 ordered-set-4.1.0 python-docx-1.2.0 reportlab-4.4.5\n"
          ]
        }
      ],
      "source": [
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c138843f",
      "metadata": {
        "id": "c138843f"
      },
      "source": [
        "## 2) Load Secrets (GOOGLE_API_KEY) from Colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "c88bba18",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c88bba18",
        "outputId": "e7908360-aa5e-4cf8-db28-80d1048f8bb1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚ö†Ô∏è Secret 'SESSION_SECRET' not found. Using fallback value.\n",
            "‚ö†Ô∏è Secret 'FLASK_API_KEY' not found. Using fallback value.\n",
            "‚úÖ Secrets loaded into environment.\n"
          ]
        }
      ],
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "from google.colab.userdata import SecretNotFoundError\n",
        "from cryptography.fernet import Fernet # Import Fernet\n",
        "\n",
        "# Helper function to get secrets with graceful fallback\n",
        "def get_secret_or_fallback(key, fallback_value):\n",
        "    try:\n",
        "        return userdata.get(key)\n",
        "    except SecretNotFoundError:\n",
        "        print(f\"‚ö†Ô∏è Secret '{key}' not found. Using fallback value.\")\n",
        "        return fallback_value\n",
        "\n",
        "# Generate a default Fernet key for SESSION_SECRET if not provided\n",
        "default_session_secret = Fernet.generate_key().decode()\n",
        "\n",
        "os.environ['GOOGLE_API_KEY'] = userdata.get('GOOGLE_API_KEY') or ''\n",
        "assert os.environ['GOOGLE_API_KEY'], \"GOOGLE_API_KEY is not set. Add it in Runtime -> Run time settings -> Secrets.\"\n",
        "os.environ['SESSION_SECRET'] = get_secret_or_fallback('SESSION_SECRET', default_session_secret)\n",
        "os.environ['FLASK_API_KEY'] = get_secret_or_fallback('FLASK_API_KEY', 'flask_api_key_fallback')\n",
        "print(\"‚úÖ Secrets loaded into environment.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9b77308e",
      "metadata": {
        "id": "9b77308e"
      },
      "source": [
        "## 3) Create Folder Structure & __init__.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "3d640d61",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3d640d61",
        "outputId": "75629d72-4662-4d41-c8e8-8f56585d4fd5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Folders and __init__.py files created.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import os\n",
        "folders = ['project','project/core','project/memory','project/tools','project/agents','project/ui','project/tests']\n",
        "for p in folders:\n",
        "    os.makedirs(p, exist_ok=True)\n",
        "    with open(os.path.join(p,'__init__.py'), 'w', encoding='utf-8') as f:\n",
        "        f.write('# Package initializer')\n",
        "print('‚úÖ Folders and __init__.py files created.')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9b465302",
      "metadata": {
        "id": "9b465302"
      },
      "source": [
        "## 4) Write Core Modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "7a4c9ad5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7a4c9ad5",
        "outputId": "66863bc4-43c1-4cee-9481-0f1315895312"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing project/core/context_engineering.py\n"
          ]
        }
      ],
      "source": [
        "\n",
        "%%writefile project/core/context_engineering.py\n",
        "import re\n",
        "PRIVACY_DISCLAIMER = (\n",
        "    \"Privacy Notice: Your input may contain sensitive legal information. \"\n",
        "    \"We do not persist document contents.\"\n",
        ")\n",
        "\n",
        "def sanitize_input(text: str) -> str:\n",
        "    text = re.sub(r\"<[^>]*>\", \"\", str(text or \"\"))\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    return text[:10000]\n",
        "\n",
        "class ContextEngine:\n",
        "    def build_context(self, user_input, session_data=None, document_content=None):\n",
        "        parts = []\n",
        "        parts.append(f\"Context(session={session_data or {}})\")\n",
        "        parts.append(f\"Input={sanitize_input(user_input)}\")\n",
        "        if document_content:\n",
        "            parts.append(f\"Document={sanitize_input(document_content)}\")\n",
        "        return \" \".join(parts)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "a84a0237",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a84a0237",
        "outputId": "d71df03a-d430-4b9d-b53a-49b60575fe9f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing project/core/observability.py\n"
          ]
        }
      ],
      "source": [
        "\n",
        "%%writefile project/core/observability.py\n",
        "import logging, json\n",
        "from datetime import datetime\n",
        "logging.basicConfig(level=logging.INFO, format='%(message)s')\n",
        "class Observability:\n",
        "    @staticmethod\n",
        "    def log(event, payload=None, contains_pii=False):\n",
        "        safe_payload = {'detail': '[REDACTED]'} if contains_pii else (payload or {})\n",
        "        record = {\n",
        "            'ts': datetime.utcnow().isoformat(),\n",
        "            'event': event,\n",
        "            'payload': safe_payload,\n",
        "        }\n",
        "        logging.info(json.dumps(record))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "cb3402a1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cb3402a1",
        "outputId": "5ff838dc-adef-4459-e161-fe66d82a2123"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing project/core/a2a_protocol.py\n"
          ]
        }
      ],
      "source": [
        "\n",
        "%%writefile project/core/a2a_protocol.py\n",
        "import uuid\n",
        "from datetime import datetime\n",
        "\n",
        "def create_message(sender, receiver, payload):\n",
        "    return {\n",
        "        'task_id': str(uuid.uuid4()),\n",
        "        'sender': sender,\n",
        "        'receiver': receiver,\n",
        "        'payload': payload,\n",
        "        'timestamp': datetime.utcnow().isoformat()\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a45dfdb4",
      "metadata": {
        "id": "a45dfdb4"
      },
      "source": [
        "## 5) Write Memory & Tools (with DOCX support)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "eec76277",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eec76277",
        "outputId": "bf8bcbb5-ebee-4c86-dd5b-47f41aefaae9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing project/memory/session_memory.py\n"
          ]
        }
      ],
      "source": [
        "\n",
        "%%writefile project/memory/session_memory.py\n",
        "import os\n",
        "from cryptography.fernet import Fernet\n",
        "class SessionMemory:\n",
        "    def __init__(self):\n",
        "        key = os.getenv('SESSION_SECRET')\n",
        "        self._key = key.encode() if key else Fernet.generate_key()\n",
        "        self._fernet = Fernet(self._key)\n",
        "        self._store = {}\n",
        "    def store(self, k, v):\n",
        "        self._store[k] = self._fernet.encrypt(str(v).encode())\n",
        "    def retrieve(self, k):\n",
        "        return self._fernet.decrypt(self._store[k]).decode() if k in self._store else None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "2d60a10e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2d60a10e",
        "outputId": "ccf5564e-b055-47c8-edfc-163a5000c2f2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing project/tools/tools.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile project/tools/tools.py\n",
        "# project/tools/tools.py\n",
        "import os\n",
        "import ast\n",
        "import operator\n",
        "import requests\n",
        "import time\n",
        "import google.generativeai as genai\n",
        "from PyPDF2 import PdfReader\n",
        "from docx import Document as DocxDocument\n",
        "\n",
        "# --- Utilities ---\n",
        "def retry_generic(func, retries=3, delay=2, exceptions=(Exception,)):\n",
        "    for attempt in range(retries):\n",
        "        try:\n",
        "            return func()\n",
        "        except exceptions:\n",
        "            if attempt < retries - 1:\n",
        "                time.sleep(delay)\n",
        "            else:\n",
        "                raise\n",
        "\n",
        "def summarizer(text, max_len=200):\n",
        "    text = str(text or \"\")\n",
        "    return text[:max_len] + '...' if len(text) > max_len else text\n",
        "\n",
        "# --- Calculator ---\n",
        "class SafeCalculator:\n",
        "    OPS = {ast.Add: operator.add, ast.Sub: operator.sub, ast.Mult: operator.mul, ast.Div: operator.truediv}\n",
        "\n",
        "    @classmethod\n",
        "    def evaluate(cls, expr):\n",
        "        try:\n",
        "            node = ast.parse(expr, mode='eval').body\n",
        "            return cls._eval(node)\n",
        "        except Exception:\n",
        "            return 'Invalid expression'\n",
        "\n",
        "    @classmethod\n",
        "    def _eval(cls, node):\n",
        "        if isinstance(node, ast.Num):\n",
        "            return node.n\n",
        "        if isinstance(node, ast.BinOp):\n",
        "            return cls.OPS[type(node.op)](cls._eval(node.left), cls._eval(node.right))\n",
        "        if isinstance(node, ast.UnaryOp):\n",
        "            if isinstance(node.op, ast.UAdd):\n",
        "                return +cls._eval(node.operand)\n",
        "            if isinstance(node.op, ast.USub):\n",
        "                return -cls._eval(node.operand)\n",
        "        raise ValueError('Unsupported')\n",
        "\n",
        "# --- Simple local search ---\n",
        "class SimpleSearch:\n",
        "    def __init__(self, corpus=None):\n",
        "        self.corpus = corpus or []\n",
        "\n",
        "    def add(self, doc_id, text):\n",
        "        self.corpus.append({\"id\": doc_id, \"text\": str(text or \"\")})\n",
        "\n",
        "    def query(self, q, top_k=3):\n",
        "        ql = [w for w in str(q or \"\").lower().split() if w]\n",
        "        scored = []\n",
        "        for item in self.corpus:\n",
        "            textl = item[\"text\"].lower()\n",
        "            score = sum(textl.count(w) for w in ql)\n",
        "            if score:\n",
        "                scored.append((score, item[\"id\"], item[\"text\"]))\n",
        "        return sorted(scored, key=lambda x: -x[0])[:top_k]\n",
        "\n",
        "# --- Domain tools ---\n",
        "class DomainTools:\n",
        "    VISA_KEYWORDS = ['visa', 'residence', 'permit', 'work', 'study', 'family', 'application', 'document']\n",
        "\n",
        "    @classmethod\n",
        "    def extract_visa_requirements(cls, text):\n",
        "        t = str(text or \"\").lower()\n",
        "        found = [k for k in cls.VISA_KEYWORDS if k in t]\n",
        "        return {\"has_visa_context\": bool(found), \"matched_keywords\": found}\n",
        "\n",
        "# --- Translator ---\n",
        "class GoogleTranslator:\n",
        "    ENDPOINT = 'https://translation.googleapis.com/language/translate/v2'\n",
        "\n",
        "    def __init__(self):\n",
        "        self.api_key = os.getenv('GOOGLE_API_KEY')\n",
        "        if not self.api_key:\n",
        "            raise RuntimeError('GOOGLE_API_KEY missing')\n",
        "\n",
        "    def translate(self, text, target='en', source='auto'):\n",
        "        def do():\n",
        "            resp = requests.post(\n",
        "                self.ENDPOINT,\n",
        "                params={'key': self.api_key},\n",
        "                json={'q': text, 'target': target, 'source': source, 'format': 'text'},\n",
        "                timeout=10,\n",
        "            )\n",
        "            resp.raise_for_status()\n",
        "            return resp.json()['data']['translations'][0]['translatedText']\n",
        "        return retry_generic(do)\n",
        "\n",
        "# --- File extractors ---\n",
        "def extract_pdf_text(pdf_path):\n",
        "    # Temporarily remove broad exception handling for debugging\n",
        "    reader = PdfReader(pdf_path)\n",
        "    pages = []\n",
        "    for page in reader.pages:\n",
        "        pages.append(page.extract_text() or \"\")\n",
        "    return \"\".join(pages)\n",
        "\n",
        "def extract_docx_text(docx_path):\n",
        "    try:\n",
        "        doc = DocxDocument(docx_path)\n",
        "        return \"\".join(p.text for p in doc.paragraphs)\n",
        "    except Exception:\n",
        "        return \"\"\n",
        "\n",
        "# --- Gemini LLM ---\n",
        "class GeminiLLM:\n",
        "    MAX_RESPONSE_LENGTH = 2000 # Define max response length\n",
        "\n",
        "    def __init__(self):\n",
        "        api_key = os.getenv('GOOGLE_API_KEY')\n",
        "        if not api_key:\n",
        "            raise RuntimeError('GOOGLE_API_KEY missing for GeminiLLM')\n",
        "        genai.configure(api_key=api_key)\n",
        "        self.model = genai.GenerativeModel('gemini-2.5-flash-lite')\n",
        "\n",
        "    def generate_response(self, user_question_original, user_question_en=None, document_content_en=None, citations=None, document_content_original=None, reply_language='en'):\n",
        "        def _call():\n",
        "            from project.ui.i18n import get_language_name\n",
        "            prompt_parts = [\n",
        "                \"You are an expert legal aid advisor for expats. \",\n",
        "                \"If document translations are provided, use them for reasoning; \",\n",
        "                \"otherwise, reason directly over the original language content. \",\n",
        "            ]\n",
        "            prompt_parts.append(f\"Original user question (language hint): {summarizer(user_question_original, 200)}\")\n",
        "            if user_question_en:\n",
        "                prompt_parts.append(f\"English-translated user question: {summarizer(user_question_en, 500)}\")\n",
        "            if document_content_en:\n",
        "                prompt_parts.append(f\"Translated legal document (for user's preferred language): {summarizer(document_content_en, 2000)}\")\n",
        "            elif document_content_original:\n",
        "                prompt_parts.append(f\"Original-language legal document (context): {summarizer(document_content_original, 2000)}\")\n",
        "            if citations:\n",
        "                prompt_parts.append(f\"Relevant excerpts: {summarizer(' | '.join(citations), 500)}\")\n",
        "            prompt_parts.append(\"Provide a precise, structured, and legally sound answer. Cite relevant parts when possible.\")\n",
        "            # FIX #3: Removed contradictory \"Always respond in the language of the original user question\"\n",
        "            # Now only enforce the preferred reply language\n",
        "            lang_name = get_language_name(reply_language)\n",
        "            prompt_parts.append(f\"Reply ONLY in {lang_name}. Do not use any other language.\")\n",
        "            response = self.model.generate_content(\"\".join(prompt_parts))\n",
        "\n",
        "            # Summarize the final response to ensure it's within limits\n",
        "            return summarizer(response.text, self.MAX_RESPONSE_LENGTH)\n",
        "        try:\n",
        "            return retry_generic(_call)\n",
        "        except Exception:\n",
        "            return \"Unable to generate response at this time.\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c79ab36a",
      "metadata": {
        "id": "c79ab36a"
      },
      "source": [
        "## 6) Write Agents & Main Orchestrator (translation fallback)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "82e7c9b4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "82e7c9b4",
        "outputId": "871b0726-4336-4b3d-b14d-a54bb05a8de0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing project/agents/planner.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile project/agents/planner.py\n",
        "from project.core.context_engineering import sanitize_input\n",
        "class Planner:\n",
        "    def plan(self, user_input, document_content=None, document_language='auto', preferred_language='en'):\n",
        "        return {\n",
        "            \"tasks\": [{\n",
        "                \"action\": \"process\",\n",
        "                \"details\": {\n",
        "                    \"user_input\": sanitize_input(user_input),\n",
        "                    \"document\": sanitize_input(document_content) if document_content else None,\n",
        "                    \"document_language\": document_language,\n",
        "                    \"preferred_language\": preferred_language\n",
        "                }\n",
        "            }]\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "4c9f1228",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4c9f1228",
        "outputId": "a5d6f2a4-734b-4ae5-fff9-e16dc111ab60"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing project/agents/evaluator.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile project/agents/evaluator.py\n",
        "# project/agents/evaluator.py\n",
        "import re\n",
        "\n",
        "class Evaluator:\n",
        "    def _estimate_confidence(self, text):\n",
        "        txt = str(text or '')\n",
        "        if not txt.strip() or len(txt.strip()) < 20:\n",
        "            return 0.55\n",
        "        if len(txt) > 500:\n",
        "            return 0.92\n",
        "        if 'keyword' in txt.lower() or 'found' in txt.lower():\n",
        "            return 0.88\n",
        "        return 0.80\n",
        "\n",
        "    def _polish_text(self, raw):\n",
        "        if not raw:\n",
        "            return 'I could not generate a meaningful answer based on the provided information.'\n",
        "        cleaned = re.sub(r'(?i)validated response: ?', '', str(raw)).strip()\n",
        "        cleaned = cleaned.replace('Document processed.', 'After reviewing your document,')\n",
        "        if not cleaned.lower().startswith(('here', 'after', 'based', 'i')):\n",
        "            cleaned = \"Here is my assessment: \" + cleaned\n",
        "        return cleaned\n",
        "\n",
        "    def evaluate(self, result):\n",
        "        return {'response': self._polish_text(result), 'confidence': round(self._estimate_confidence(result), 2)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "d502d58f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d502d58f",
        "outputId": "401144be-90cf-4e40-f4a5-7e8d31284ed7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing project/agents/worker.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile project/agents/worker.py\n",
        "# project/agents/worker.py\n",
        "from project.tools.tools import GoogleTranslator, GeminiLLM, SimpleSearch, DomainTools\n",
        "from project.core.context_engineering import ContextEngine\n",
        "from langdetect import detect, LangDetectException\n",
        "\n",
        "class Worker:\n",
        "    def __init__(self):\n",
        "        self.llm = GeminiLLM()\n",
        "        self.context_engine = ContextEngine()\n",
        "        self.search = SimpleSearch()\n",
        "\n",
        "    def _detect_language(self, text):\n",
        "        \"\"\"Auto-detect language using langdetect. Returns language code or 'en' on failure.\"\"\"\n",
        "        if not text or len(text) < 3:\n",
        "            return 'en'\n",
        "        try:\n",
        "            detected = detect(text)\n",
        "            # Map common codes: pt->es, zh-cn->auto, etc.\n",
        "            lang_map = {'pt': 'es', 'zh-cn': 'en', 'zh-tw': 'en'}\n",
        "            return lang_map.get(detected, detected)\n",
        "        except LangDetectException:\n",
        "            return 'en'\n",
        "\n",
        "    def _translate_safe(self, text, target='en'):\n",
        "        try:\n",
        "            if not text:\n",
        "                return None\n",
        "            translator = GoogleTranslator()\n",
        "            return translator.translate(text, target=target)\n",
        "        except Exception as e:\n",
        "            # Log but don't raise; fallback to original\n",
        "            print(f\"‚ö†Ô∏è Translation failed: {str(e)}\")\n",
        "            return None\n",
        "\n",
        "    def execute(self, task):\n",
        "        a = task.get(\"action\")\n",
        "        d = task.get(\"details\", {})\n",
        "        if a != \"process\":\n",
        "            return \"Unknown action\"\n",
        "\n",
        "        user_input_original = d.get(\"user_input\", \"\")\n",
        "        document = d.get(\"document\")\n",
        "        document_language = d.get(\"document_language\", \"auto\")\n",
        "        preferred_language = d.get(\"preferred_language\", 'en')\n",
        "\n",
        "        # Auto-detect document language if 'auto' is specified\n",
        "        if document_language == \"auto\" and document:\n",
        "            document_language = self._detect_language(document)\n",
        "            print(f\"üìç Auto-detected document language: {document_language}\")\n",
        "\n",
        "        # Translate question to English for internal reasoning\n",
        "        user_input_en = self._translate_safe(user_input_original, target='en')\n",
        "\n",
        "        # FIX #1 & #4: Translate document to preferred language (for user communication)\n",
        "        # Also use document_language as source hint for better translation\n",
        "        document_translated = self._translate_safe(document, target=preferred_language) if document else None\n",
        "\n",
        "        # Also translate to English for search/reasoning if preferred language is not English\n",
        "        document_en = self._translate_safe(document, target='en') if document and preferred_language != 'en' else document_translated\n",
        "\n",
        "        # Build local search corpus using English content for better search accuracy\n",
        "        doc_for_search = document_en or document or \"\"\n",
        "        if doc_for_search:\n",
        "            self.search.add(\"doc\", doc_for_search)\n",
        "        citations = [t for _, _, t in self.search.query(user_input_en or user_input_original, top_k=2)]\n",
        "        domain_info = DomainTools.extract_visa_requirements(doc_for_search)\n",
        "\n",
        "        # LLM generation: Pass translated document and enforced reply language\n",
        "        # Use document_translated (in preferred language) for reasoning\n",
        "        base = self.llm.generate_response(\n",
        "            user_question_original=user_input_original,\n",
        "            user_question_en=user_input_en,\n",
        "            document_content_en=document_translated,\n",
        "            citations=citations,\n",
        "            document_content_original=document if (document and not document_translated) else None,\n",
        "            reply_language=preferred_language\n",
        "        )\n",
        "\n",
        "        if domain_info.get(\"has_visa_context\"):\n",
        "            matched = ', '.join(domain_info.get('matched_keywords', []))\n",
        "            base = f\"{base}\\n\\n(Detected visa-related context: {matched})\"\n",
        "\n",
        "        return base\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "185beadc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "185beadc",
        "outputId": "32fa16db-6640-488f-fa7e-ae07ad3a599a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing project/main_agent.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile project/main_agent.py\n",
        "# project/main_agent.py\n",
        "from project.agents.planner import Planner\n",
        "from project.agents.worker import Worker\n",
        "from project.agents.evaluator import Evaluator\n",
        "from project.core.context_engineering import PRIVACY_DISCLAIMER\n",
        "from project.core.observability import Observability\n",
        "from project.core.a2a_protocol import create_message\n",
        "from project.memory.session_memory import SessionMemory\n",
        "\n",
        "class MainAgent:\n",
        "    def __init__(self):\n",
        "        self.planner = Planner()\n",
        "        self.worker = Worker()\n",
        "        self.evaluator = Evaluator()\n",
        "        self.memory = SessionMemory()\n",
        "\n",
        "    def handle_message(self, user_input, document_content=None, document_language='auto', preferred_language='en'):\n",
        "        Observability.log('start', {'input_len': len(str(user_input))}, contains_pii=True)\n",
        "        plan = self.planner.plan(user_input, document_content, document_language, preferred_language)\n",
        "        task = plan[\"tasks\"][0]\n",
        "        msg = create_message('planner', 'worker', task)\n",
        "        Observability.log('a2a_msg', {'id': msg['task_id'], 'from': msg['sender'], 'to': msg['receiver']})\n",
        "        result = self.worker.execute(task)\n",
        "        msg2 = create_message('worker', 'evaluator', {'result_preview': str(result)[:60]})\n",
        "        Observability.log('a2a_msg', {'id': msg2['task_id'], 'from': msg2['sender'], 'to': msg2['receiver']})\n",
        "        eval_result = self.evaluator.evaluate(result)\n",
        "        eval_result['response'] = f\"{eval_result.get('response','')}\\n\\n{PRIVACY_DISCLAIMER}\"\n",
        "        self.memory.store('last_question', user_input)\n",
        "        self.memory.store('last_response', eval_result['response'])\n",
        "        Observability.log('end', {'confidence': eval_result['confidence']})\n",
        "        return eval_result\n",
        "\n",
        "def run_agent(user_input, document_content=None, document_language='auto', preferred_language='en'):\n",
        "    return MainAgent().handle_message(user_input, document_content, document_language, preferred_language)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8de735e4",
      "metadata": {
        "id": "8de735e4"
      },
      "source": [
        "## 7) UI Strings, API & Demo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "e1a20750",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e1a20750",
        "outputId": "2c98c336-6e62-4a65-b32d-a1874b6a0a05"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing project/ui/i18n.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile project/ui/i18n.py\n",
        "TRANSLATIONS = {\n",
        "    'en': {'title': 'Expat Legal Aid Advisor', 'welcome': 'Welcome', 'disclaimer': 'Privacy Notice'},\n",
        "    'es': {'title': 'Asesor Legal', 'welcome': 'Bienvenido', 'disclaimer': 'Aviso de privacidad'},\n",
        "    'fr': {'title': 'Conseiller Juridique', 'welcome': 'Bienvenue', 'disclaimer': 'Avis de confidentialit√©'},\n",
        "    'nl': {'title': 'Expat Juridisch Advies', 'welcome': 'Welkom', 'disclaimer': 'Privacyverklaring'},\n",
        "    'de': {'title': 'Expat-Rechtsberatung', 'welcome': 'Willkommen', 'disclaimer': 'Datenschutzerkl√§rung'}\n",
        "}\n",
        "\n",
        "LANGUAGE_MAP = {\n",
        "    'en': 'English',\n",
        "    'es': 'Spanish',\n",
        "    'fr': 'French',\n",
        "    'nl': 'Dutch',\n",
        "    'de': 'German'\n",
        "}\n",
        "\n",
        "def t(key, lang='en'):\n",
        "    return TRANSLATIONS.get(lang, TRANSLATIONS['en']).get(key, key)\n",
        "\n",
        "def get_language_name(lang_code):\n",
        "    \"\"\"Return human-readable language name for LLM prompt.\"\"\"\n",
        "    return LANGUAGE_MAP.get(lang_code, lang_code)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "201ef55e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "201ef55e",
        "outputId": "12f0fd1b-cad6-47aa-ecb6-fbaa87d8868f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing project/app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile project/app.py\n",
        "from flask import Flask, request, jsonify\n",
        "from flask_httpauth import HTTPBasicAuth\n",
        "from flask_limiter import Limiter\n",
        "from flask_limiter.util import get_remote_address\n",
        "import os\n",
        "from project.main_agent import run_agent\n",
        "from project.core.context_engineering import PRIVACY_DISCLAIMER\n",
        "\n",
        "\n",
        "def create_app():\n",
        "    app = Flask(__name__)\n",
        "    auth = HTTPBasicAuth()\n",
        "    limiter = Limiter(app=app, key_func=get_remote_address, storage_uri='memory:///')\n",
        "\n",
        "    @auth.verify_password\n",
        "    def verify_password(username, password):\n",
        "        api_key = request.headers.get('X-API-Key') or password\n",
        "        if api_key and api_key == os.getenv('FLASK_API_KEY'):\n",
        "            return username\n",
        "        return None\n",
        "\n",
        "    @app.route('/query', methods=['POST'])\n",
        "    @auth.login_required\n",
        "    @limiter.limit('10 per minute')\n",
        "    def query():\n",
        "        data = request.json or {}\n",
        "        agent_response = run_agent(data.get('input', ''), data.get('document_content'))\n",
        "        return jsonify({'response': agent_response, 'privacy': PRIVACY_DISCLAIMER})\n",
        "\n",
        "    @app.errorhandler(429)\n",
        "    def ratelimit_handler(e):\n",
        "        return jsonify({'code': 429, 'name': 'Rate Limit Exceeded', 'description': 'You have exceeded your rate limit.'}), 429\n",
        "\n",
        "    return app\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    app = create_app()\n",
        "    app.run(host='0.0.0.0', port=5000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "e89cd070",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e89cd070",
        "outputId": "6ad3e12c-0178-4248-f62f-326756a4408e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing project/run_demo.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile project/run_demo.py\n",
        "import os\n",
        "import sys\n",
        "from unittest.mock import patch\n",
        "\n",
        "print(\"DEBUG: Starting run_demo.py script\", flush=True)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    print(\"DEBUG: Inside __main__ block.\", flush=True)\n",
        "    if os.getenv('E2E_TEST_MODE') == 'true':\n",
        "        print(\"DEBUG: E2E_TEST_MODE is ON. Applying mock.\", flush=True)\n",
        "        try:\n",
        "            # The patch needs to wrap the execution of run_agent\n",
        "            with patch('project.tools.tools.GeminiLLM') as MockGeminiLLM: # Patch the actual class used by Worker\n",
        "                mock = MockGeminiLLM.return_value\n",
        "                mock.generate_response.return_value = 'Mocked LLM response for Hello! This is a demo.'\n",
        "                print(\"DEBUG: GeminiLLM mocked.\", flush=True)\n",
        "                from project.main_agent import run_agent # Import here to ensure it uses the patched GeminiLLM when instantiated\n",
        "                print(\"Running agent...\", flush=True)\n",
        "                result = run_agent('Hello! This is a demo.')\n",
        "                print(\"DEBUG: Agent run completed.\", flush=True)\n",
        "                print(result.get('response', 'Error: No response key in agent output.'), flush=True)\n",
        "                print(\"Agent run finished.\", flush=True)\n",
        "        except Exception as e:\n",
        "            print(f\"ERROR DURING MOCKED AGENT EXECUTION: {e}\", file=sys.stderr, flush=True)\n",
        "            sys.exit(1)\n",
        "    else:\n",
        "        print(\"DEBUG: E2E_TEST_MODE is OFF. Running real agent.\", flush=True)\n",
        "        from project.main_agent import run_agent\n",
        "        print(\"Running agent...\", flush=True)\n",
        "        try:\n",
        "            result = run_agent('Hello! This is a demo.')\n",
        "            print(\"DEBUG: Agent run completed.\", flush=True)\n",
        "            print(result.get('response', 'Error: No response key in agent output.'), flush=True)\n",
        "            print(\"Agent run finished.\", flush=True)\n",
        "        except Exception as e:\n",
        "            print(f\"ERROR DURING REAL AGENT EXECUTION: {e}\", file=sys.stderr, flush=True)\n",
        "            sys.exit(1)\n",
        "\n",
        "print(\"DEBUG: End of run_demo.py script.\", flush=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "21745f74",
      "metadata": {
        "id": "21745f74"
      },
      "source": [
        "## 8) Coverage & README"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "2726d301",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2726d301",
        "outputId": "732d2df7-5851-4609-8e36-5c5129c7360a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing .coveragerc\n"
          ]
        }
      ],
      "source": [
        "\n",
        "%%writefile .coveragerc\n",
        "[run]\n",
        "source = project\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "43ac9c7b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "43ac9c7b",
        "outputId": "debcf357-6177-42f7-faf4-15bcfb394b9a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing README.md\n"
          ]
        }
      ],
      "source": [
        "\n",
        "%%writefile README.md\n",
        "# Expat Legal Aid Advisor (Colab-ready)\n",
        "Full multi-agent project with tests, coverage, Flask API, and Gradio UI (mandatory).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cd92e58c",
      "metadata": {
        "id": "cd92e58c"
      },
      "source": [
        "## 9) Tests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "c8d1dc94",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c8d1dc94",
        "outputId": "072678a6-6070-42b0-c676-934eda1cbd2f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing project/tests/test_unit.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile project/tests/test_unit.py\n",
        "import os, requests, pytest\n",
        "from unittest.mock import patch\n",
        "from project.agents.worker import Worker\n",
        "from project.agents.evaluator import Evaluator\n",
        "from project.main_agent import run_agent\n",
        "from project.core.context_engineering import sanitize_input, ContextEngine\n",
        "from project.core.observability import Observability\n",
        "from project.core.a2a_protocol import create_message\n",
        "from project.memory.session_memory import SessionMemory\n",
        "from project.tools.tools import SafeCalculator, summarizer, GoogleTranslator, GeminiLLM, SimpleSearch, DomainTools, extract_pdf_text\n",
        "import logging\n",
        "\n",
        "def test_evaluator_confidence_and_polish():\n",
        "    e = Evaluator()\n",
        "    assert e.evaluate('short')['confidence'] >= 0.55\n",
        "    long = 'a' * 600\n",
        "    assert e._estimate_confidence(long) == 0.92\n",
        "    assert e._polish_text('') == 'I could not generate a meaningful answer based on the provided information.'\n",
        "    assert e._polish_text('Document processed. Some text.').startswith('After reviewing your document,')\n",
        "\n",
        "def test_planner_action_and_sanitize():\n",
        "    from project.agents.planner import Planner\n",
        "    plan = Planner().plan('<b>Hi</b>')\n",
        "    assert plan['tasks'][0]['action'] == 'process'\n",
        "    assert sanitize_input('<i>x</i>') == 'x'\n",
        "\n",
        "def test_planner_with_language_parameters():\n",
        "    \"\"\"Test that Planner correctly passes language parameters.\"\"\"\n",
        "    from project.agents.planner import Planner\n",
        "    p = Planner()\n",
        "    plan = p.plan('¬øHola?', None, 'es', 'es')\n",
        "    assert plan['tasks'][0]['details']['document_language'] == 'es'\n",
        "    assert plan['tasks'][0]['details']['preferred_language'] == 'es'\n",
        "\n",
        "def test_context_engine_builds_context():\n",
        "    ctx = ContextEngine().build_context('Q?', {'user': 'alice'}, 'doc text')\n",
        "    assert \"Context(session={'user': 'alice'})\" in ctx\n",
        "    assert 'Input=Q?' in ctx\n",
        "    assert 'Document=doc text' in ctx\n",
        "\n",
        "def test_observability_logs(caplog):\n",
        "    caplog.set_level(logging.INFO) # Set logging level to INFO\n",
        "    Observability.log('start', {'a': 1})\n",
        "    assert any('\"event\": \"start\"' in rec.message for rec in caplog.records)\n",
        "\n",
        "def test_a2a_message_has_fields():\n",
        "    m = create_message('planner', 'worker', {'x': 1})\n",
        "    assert {'task_id','sender','receiver','payload','timestamp'}.issubset(m.keys())\n",
        "\n",
        "def test_session_memory_with_secret(monkeypatch):\n",
        "    from cryptography.fernet import Fernet\n",
        "    key = Fernet.generate_key().decode()\n",
        "    monkeypatch.setenv('SESSION_SECRET', key)\n",
        "    mem = SessionMemory()\n",
        "    mem.store('k','v')\n",
        "    assert mem.retrieve('k') == 'v'\n",
        "\n",
        "def test_session_memory_without_secret(monkeypatch):\n",
        "    monkeypatch.delenv('SESSION_SECRET', raising=False)\n",
        "    mem = SessionMemory()\n",
        "    mem.store('k','v')\n",
        "    assert mem.retrieve('k') == 'v'\n",
        "\n",
        "def test_tools_calculator_and_summarizer():\n",
        "    assert SafeCalculator.evaluate('1+2') == 3\n",
        "    assert SafeCalculator.evaluate('+10') == 10\n",
        "    assert 'Invalid' in SafeCalculator.evaluate('foo(1)')\n",
        "    assert summarizer('x'*300).endswith('...')\n",
        "\n",
        "def test_simple_search_and_domain_tools():\n",
        "    s = SimpleSearch()\n",
        "    s.add('d1', 'visa application requires documents')\n",
        "    s.add('d2', 'residence permit and study visa')\n",
        "    hits = s.query('visa application', top_k=2)\n",
        "    assert len(hits) >= 1\n",
        "    info = DomainTools.extract_visa_requirements('You need a work permit and visa.')\n",
        "    assert info['has_visa_context'] and 'visa' in info['matched_keywords']\n",
        "\n",
        "def test_extract_pdf_text(tmp_path):\n",
        "    from reportlab.pdfgen import canvas\n",
        "    pdf_path = tmp_path / 'test.pdf'\n",
        "    c = canvas.Canvas(str(pdf_path)) # Convert PosixPath to string\n",
        "    c.drawString(100, 750, 'This is a PDF test for extraction.')\n",
        "    c.save()\n",
        "    text = extract_pdf_text(str(pdf_path))\n",
        "    assert 'PDF test for extraction' in text\n",
        "\n",
        "@patch('project.agents.worker.GeminiLLM')\n",
        "@patch('project.agents.worker.GoogleTranslator')\n",
        "def test_worker_translates_q_and_doc_and_passes_citations(MockTrans, MockLLM):\n",
        "    mock_llm = MockLLM.return_value\n",
        "    mock_llm.generate_response.return_value = 'LLM generated response'\n",
        "    mock_trans = MockTrans.return_value\n",
        "    mock_trans.translate.side_effect = ['Translated Question', 'Translated Document']\n",
        "    os.environ['GOOGLE_API_KEY'] = 'fake'\n",
        "    w = Worker()\n",
        "    task = {'action':'process','details':{'user_input':'¬øRequisitos?', 'document':'documento en espa√±ol', 'document_language': 'es', 'preferred_language': 'es'}}\n",
        "    out = w.execute(task)\n",
        "    assert 'LLM generated response' in out # Updated assertion\n",
        "    assert mock_trans.translate.call_count >= 2\n",
        "    args, kwargs = mock_llm.generate_response.call_args\n",
        "    assert 'citations' in kwargs\n",
        "    del os.environ['GOOGLE_API_KEY']\n",
        "\n",
        "@patch('project.agents.worker.GeminiLLM')\n",
        "@patch('project.agents.worker.GoogleTranslator')\n",
        "def test_worker_domain_hint_appended(MockTrans, MockLLM):\n",
        "    mock_llm = MockLLM.return_value\n",
        "    mock_llm.generate_response.return_value = 'LLM generated response'\n",
        "    mock_trans = MockTrans.return_value\n",
        "    mock_trans.translate.side_effect = ['Translated Question', 'Translated Document mentioning visa']\n",
        "    os.environ['GOOGLE_API_KEY'] = 'fake'\n",
        "    w = Worker()\n",
        "    task = {'action':'process','details':{'user_input':'¬øRequisitos?', 'document':'visa doc', 'document_language': 'auto', 'preferred_language': 'es'}}\n",
        "    out = w.execute(task)\n",
        "    assert 'Detected visa-related context' in out\n",
        "    del os.environ['GOOGLE_API_KEY']\n",
        "\n",
        "def test_worker_unknown_action():\n",
        "    with patch('project.agents.worker.GeminiLLM'):\n",
        "        w = Worker()\n",
        "    assert w.execute({'action': 'translate', 'details': {'user_input':'hi'}}) == 'Unknown action'\n",
        "\n",
        "def test_worker_language_auto_detection():\n",
        "    \"\"\"Test that Worker detects document language when 'auto' is specified.\"\"\"\n",
        "    with patch('project.agents.worker.GeminiLLM'):\n",
        "        w = Worker()\n",
        "        # Spanish text\n",
        "        spanish_text = 'Este es un documento en espa√±ol sobre visa de trabajo.'\n",
        "        detected = w._detect_language(spanish_text)\n",
        "        assert detected in ['es', 'en']  # Might detect as Spanish or fallback to English\n",
        "\n",
        "@patch('project.tools.tools.requests')\n",
        "def test_google_translator_success(mock_requests):\n",
        "    mock_requests.post.return_value.raise_for_status.return_value = None\n",
        "    mock_requests.post.return_value.json.return_value = {'data': {'translations':[{'translatedText':'Hola'}]}}\n",
        "    os.environ['GOOGLE_API_KEY'] = 'key'\n",
        "    tr = GoogleTranslator()\n",
        "    assert tr.translate('Hello', 'es') == 'Hola'\n",
        "    del os.environ['GOOGLE_API_KEY']\n",
        "\n",
        "@patch('project.tools.tools.requests')\n",
        "def test_google_translator_api_error(mock_requests):\n",
        "    mock_requests.post.return_value.raise_for_status.side_effect = requests.exceptions.RequestException('API Error')\n",
        "    os.environ['GOOGLE_API_KEY'] = 'key'\n",
        "    tr = GoogleTranslator()\n",
        "    with pytest.raises(requests.exceptions.RequestException):\n",
        "        tr.translate('Hello','es')\n",
        "    del os.environ['GOOGLE_API_KEY']\n",
        "\n",
        "def test_gemini_llm_init_no_key(monkeypatch):\n",
        "    from project.tools.tools import GeminiLLM\n",
        "    monkeypatch.delenv('GOOGLE_API_KEY', raising=False)\n",
        "    with pytest.raises(RuntimeError, match='missing for GeminiLLM'):\n",
        "        GeminiLLM()\n",
        "\n",
        "@patch('google.generativeai.GenerativeModel')\n",
        "def test_gemini_llm_generate_response(MockModel, monkeypatch):\n",
        "    from project.tools.tools import GeminiLLM\n",
        "    mock = MockModel.return_value\n",
        "    mock.generate_content.return_value.text = 'Mocked'\n",
        "    monkeypatch.setenv('GOOGLE_API_KEY','k')\n",
        "    llm = GeminiLLM()\n",
        "    assert llm.generate_response('Hola?', 'Hello?') == 'Mocked'\n",
        "    mock.generate_content.side_effect = Exception('LLM down')\n",
        "    assert llm.generate_response('Hola?', 'Hello?') == 'Unable to generate response at this time.'\n",
        "    monkeypatch.delenv('GOOGLE_API_KEY', raising=False)\n",
        "\n",
        "@patch('google.generativeai.GenerativeModel')\n",
        "def test_gemini_llm_with_language_enforcement(MockModel, monkeypatch):\n",
        "    \"\"\"Test that GeminiLLM appends language enforcement to prompt.\"\"\"\n",
        "    from project.tools.tools import GeminiLLM\n",
        "    mock = MockModel.return_value\n",
        "    mock.generate_content.return_value.text = 'Spanish response'\n",
        "    monkeypatch.setenv('GOOGLE_API_KEY','k')\n",
        "    llm = GeminiLLM()\n",
        "    result = llm.generate_response('¬øHola?', reply_language='es')\n",
        "    # Verify that the prompt included language enforcement\n",
        "    call_args = mock.generate_content.call_args\n",
        "    prompt = call_args[0][0] if call_args[0] else ''\n",
        "    assert 'Spanish' in prompt or 'reply ONLY' in prompt\n",
        "    monkeypatch.delenv('GOOGLE_API_KEY', raising=False)\n",
        "\n",
        "@patch('project.agents.worker.GeminiLLM')\n",
        "@patch('project.agents.worker.GoogleTranslator')\n",
        "def test_run_agent_returns_polished_dict(MockTrans, MockLLM, monkeypatch):\n",
        "    MockLLM.return_value.generate_response.return_value = 'Mocked LLM response for Hello!'\n",
        "    MockTrans.return_value.translate.side_effect = ['Translated Question']\n",
        "    monkeypatch.setenv('GOOGLE_API_KEY','k')\n",
        "    out = run_agent('Hello!')\n",
        "    assert isinstance(out, dict) and 'response' in out and 'confidence' in out\n",
        "    assert ('Here is my assessment' in out['response']) or ('After reviewing your document' in out['response'])\n",
        "    assert 'Privacy Notice' in out['response']\n",
        "    monkeypatch.delenv('GOOGLE_API_KEY', raising=False)\n",
        "\n",
        "@patch('project.agents.worker.GeminiLLM')\n",
        "@patch('project.agents.worker.GoogleTranslator')\n",
        "def test_run_agent_with_language_parameters(MockTrans, MockLLM, monkeypatch):\n",
        "    \"\"\"Test that run_agent accepts and passes language parameters.\"\"\"\n",
        "    MockLLM.return_value.generate_response.return_value = 'Spanish response'\n",
        "    MockTrans.return_value.translate.return_value = 'Translated'\n",
        "    monkeypatch.setenv('GOOGLE_API_KEY','k')\n",
        "    out = run_agent('¬øPregunta?', None, 'es', 'es')\n",
        "    assert isinstance(out, dict)\n",
        "    assert 'response' in out and 'confidence' in out\n",
        "    monkeypatch.delenv('GOOGLE_API_KEY', raising=False)\n",
        "\n",
        "@patch('project.agents.worker.GeminiLLM')\n",
        "@patch('project.agents.worker.GoogleTranslator')\n",
        "def test_worker_translates_document_to_preferred_language(MockTrans, MockLLM):\n",
        "    \"\"\"FIX #5: Test that document is translated to preferred language, not always English.\"\"\"\n",
        "    mock_llm = MockLLM.return_value # Corrected typo from MockLLm to MockLLM\n",
        "    mock_llm.generate_response.return_value = 'Spanish response about visa'\n",
        "    mock_trans = MockTrans.return_value\n",
        "    # First call: question to English, Second call: document to Spanish\n",
        "    mock_trans.translate.side_effect = ['Pregunta en ingl√©s', 'Documento traducido al espa√±ol']\n",
        "\n",
        "    os.environ['GOOGLE_API_KEY'] = 'fake'\n",
        "    w = Worker()\n",
        "    task = {\n",
        "        'action': 'process',\n",
        "        'details': {\n",
        "            'user_input': '¬øCuales son los requisitos?',\n",
        "            'document': 'Este es un documento en espa√±ol sobre visa',\n",
        "            'document_language': 'es',\n",
        "            'preferred_language': 'es'\n",
        "        }\n",
        "    }\n",
        "    out = w.execute(task)\n",
        "\n",
        "    # Verify translator was called with preferred language as target\n",
        "    assert mock_trans.translate.call_count >= 2\n",
        "    # Check that one of the calls targeted 'es' (preferred language)\n",
        "    calls = [call[1].get('target') for call in mock_trans.translate.call_args_list if call[1]]\n",
        "    assert 'es' in calls, f\"Expected 'es' in translation targets, got {calls}\"\n",
        "\n",
        "    assert 'Spanish response about visa' in out # Updated assertion\n",
        "    del os.environ['GOOGLE_API_KEY']\n",
        "\n",
        "def test_google_translator_no_api_key(monkeypatch):\n",
        "    \"\"\"GoogleTranslator should raise when GOOGLE_API_KEY is missing.\"\"\"\n",
        "    monkeypatch.delenv('GOOGLE_API_KEY', raising=False)\n",
        "    with pytest.raises(RuntimeError):\n",
        "        GoogleTranslator()\n",
        "\n",
        "@patch('project.tools.tools.requests')\n",
        "def test_google_translator_retries_on_timeout(mock_requests, monkeypatch):\n",
        "    \"\"\"Simulate a transient timeout on first POST, success on retry.\"\"\"\n",
        "    from requests.exceptions import Timeout\n",
        "    class FakeResp:\n",
        "        def raise_for_status(self):\n",
        "            return None\n",
        "        def json(self):\n",
        "            return {'data': {'translations':[{'translatedText':'Hola'}]}}\n",
        "\n",
        "    # Set API key before instantiating GoogleTranslator\n",
        "    monkeypatch.setenv('GOOGLE_API_KEY', 'key')\n",
        "\n",
        "    call_count = {'n': 0}\n",
        "    def post_side_effect(*args, **kwargs):\n",
        "        if call_count['n'] == 0:\n",
        "            call_count['n'] += 1\n",
        "            raise Timeout('simulated timeout')\n",
        "        return FakeResp()\n",
        "    with patch('project.tools.tools.requests.post', side_effect=post_side_effect):\n",
        "        tr = GoogleTranslator()\n",
        "        result = tr.translate('Hello', 'es')\n",
        "        assert result == 'Hola'\n",
        "    monkeypatch.delenv('GOOGLE_API_KEY', raising=False)\n",
        "\n",
        "@patch('project.agents.worker.GeminiLLM')\n",
        "@patch('project.agents.worker.GoogleTranslator')\n",
        "def test_worker_translate_exception_uses_original_doc(MockTrans, MockLLM):\n",
        "    \"\"\"If translator fails, Worker should pass the original document to the LLM (fallback path).\"\"\"\n",
        "    mock_llm = MockLLM.return_value\n",
        "    def capture_generate(*args, **kwargs):\n",
        "        # When translation fails document_translated is None, so document_content_original should be passed\n",
        "        assert kwargs.get('document_content_original') is not None, 'Expected original document in LLM args on translation failure'\n",
        "        return 'LLM fallback response'\n",
        "    mock_llm.generate_response.side_effect = capture_generate\n",
        "    mock_trans = MockTrans.return_value\n",
        "    # Simulate translator failing for any call\n",
        "    mock_trans.translate.side_effect = Exception('forced translator failure')\n",
        "    os.environ['GOOGLE_API_KEY'] = 'fake'\n",
        "    w = Worker()\n",
        "    task = {'action':'process','details':{'user_input':'Pregunta','document':'documento original','document_language':'es','preferred_language':'es'}}\n",
        "    out = w.execute(task)\n",
        "    assert 'LLM fallback response' in out # Updated assertion\n",
        "    del os.environ['GOOGLE_API_KEY']\n",
        "\n",
        "@patch('project.agents.worker.GeminiLLM')\n",
        "def test_worker_translate_safe_none(MockLLM):\n",
        "    \"\"\"_translate_safe should return None when given None and should not raise.\"\"\"\n",
        "    with patch('project.agents.worker.GeminiLLM'):\n",
        "        w = Worker()\n",
        "        assert w._translate_safe(None) is None\n",
        "\n",
        "def test_worker_empty_text_detection():\n",
        "    \"\"\"Test _detect_language with empty/short text returns 'en' as fallback.\"\"\"\n",
        "    with patch('project.agents.worker.GeminiLLM'):\n",
        "        w = Worker()\n",
        "        assert w._detect_language('') == 'en'\n",
        "        assert w._detect_language('a') == 'en'\n",
        "        assert w._detect_language(None) == 'en'\n",
        "\n",
        "@patch('project.agents.worker.GeminiLLM')\n",
        "def test_worker_execute_without_document(MockLLM):\n",
        "    \"\"\"Test Worker.execute when no document is provided.\"\"\"\n",
        "    mock_llm = MockLLM.return_value\n",
        "    mock_llm.generate_response.return_value = 'LLM response'\n",
        "    os.environ['GOOGLE_API_KEY'] = 'fake'\n",
        "    w = Worker()\n",
        "    task = {\n",
        "        'action': 'process',\n",
        "        'details': {\n",
        "            'user_input': 'What are visa requirements?',\n",
        "            'document': None,\n",
        "            'document_language': 'auto',\n",
        "            'preferred_language': 'en'\n",
        "        }\n",
        "    }\n",
        "    out = w.execute(task)\n",
        "    assert out == 'LLM response'\n",
        "    del os.environ['GOOGLE_API_KEY']\n",
        "\n",
        "@patch('project.agents.worker.GeminiLLM')\n",
        "@patch('project.agents.worker.GoogleTranslator')\n",
        "def test_worker_document_language_auto_detection_flow(MockTrans, MockLLM):\n",
        "    \"\"\"Test Worker auto-detects document language and translates accordingly.\"\"\"\n",
        "    mock_llm = MockLLM.return_value\n",
        "    mock_llm.generate_response.return_value = 'Analyzed response'\n",
        "    mock_trans = MockTrans.return_value\n",
        "    mock_trans.translate.side_effect = ['Question EN', 'Document EN']\n",
        "    os.environ['GOOGLE_API_KEY'] = 'fake'\n",
        "    w = Worker()\n",
        "    spanish_doc = 'Este es un documento sobre visa y permiso de residencia.'\n",
        "    task = {\n",
        "        'action': 'process',\n",
        "        'details': {\n",
        "            'user_input': '¬øRequisitos?',\n",
        "            'document': spanish_doc,\n",
        "            'document_language': 'auto',\n",
        "            'preferred_language': 'en'\n",
        "        }\n",
        "    }\n",
        "    out = w.execute(task)\n",
        "    assert 'Analyzed response' in out # Updated assertion\n",
        "    # Verify translator was called\n",
        "    assert mock_trans.translate.call_count >= 1\n",
        "    del os.environ['GOOGLE_API_KEY']\n",
        "\n",
        "def test_summarizer_edge_cases():\n",
        "    \"\"\"Test summarizer with various edge cases.\"\"\"\n",
        "    assert summarizer('') == ''\n",
        "    assert summarizer(None) == ''\n",
        "    assert summarizer('short text') == 'short text'\n",
        "    long_text = 'x' * 250\n",
        "    result = summarizer(long_text)\n",
        "    assert result.endswith('...')\n",
        "    assert len(result) == 203  # 200 + '...'\n",
        "\n",
        "@patch('project.tools.tools.requests')\n",
        "def test_google_translator_with_different_languages(mock_requests):\n",
        "    \"\"\"Test GoogleTranslator with various language pairs.\"\"\"\n",
        "    mock_requests.post.return_value.raise_for_status.return_value = None\n",
        "    mock_requests.post.return_value.json.return_value = {'data': {'translations':[{'translatedText':'Bonjour'}]}}\n",
        "    os.environ['GOOGLE_API_KEY'] = 'key'\n",
        "    tr = GoogleTranslator()\n",
        "    result = tr.translate('Hello', target='fr')\n",
        "    assert result == 'Bonjour'\n",
        "    # Verify the call was made with correct parameters\n",
        "    call_args = mock_requests.post.call_args\n",
        "    assert call_args[1]['json']['target'] == 'fr'\n",
        "    del os.environ['GOOGLE_API_KEY']\n",
        "\n",
        "def test_safe_calculator_division():\n",
        "    \"\"\"Test SafeCalculator with division operations.\"\"\"\n",
        "    assert SafeCalculator.evaluate('10/2') == 5.0\n",
        "    assert SafeCalculator.evaluate('5/2') == 2.5\n",
        "\n",
        "def test_safe_calculator_complex_expression():\n",
        "    \"\"\"Test SafeCalculator with complex expressions.\"\"\"\n",
        "    assert SafeCalculator.evaluate('2 + 3 * 4') == 14\n",
        "    assert SafeCalculator.evaluate('(2 + 3) * 4') == 20\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "a6ee32c3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a6ee32c3",
        "outputId": "e2dab8f3-2714-444f-dd1a-e5aa989f4aff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing project/tests/test_integration.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile project/tests/test_integration.py\n",
        "import os\n",
        "from unittest.mock import patch\n",
        "from project.main_agent import MainAgent\n",
        "import logging\n",
        "\n",
        "@patch('project.main_agent.Worker')\n",
        "def test_integration_with_observability_and_a2a(MockWorker, monkeypatch, caplog):\n",
        "    # Ensure caplog captures INFO level messages\n",
        "    caplog.set_level(logging.INFO)\n",
        "\n",
        "    mock_w = MockWorker.return_value\n",
        "    mock_w.execute.return_value = 'Mocked LLM response for Check visa'\n",
        "    monkeypatch.setenv('GOOGLE_API_KEY','k')\n",
        "    res = MainAgent().handle_message('Check visa')\n",
        "    assert isinstance(res, dict) and 'response' in res and 'confidence' in res\n",
        "\n",
        "    # Check for log messages. The messages are JSON strings, so we need to check if the string contains the JSON fragment.\n",
        "    assert any('\"event\": \"start\"' in r.message for r in caplog.records), \"'start' event not found in logs\"\n",
        "    assert any('\"event\": \"end\"' in r.message for r in caplog.records), \"'end' event not found in logs\"\n",
        "\n",
        "    monkeypatch.delenv('GOOGLE_API_KEY', raising=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "7a5ef4dc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7a5ef4dc",
        "outputId": "b4fd322e-6682-4269-f2cf-ab3a9d391076"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing project/tests/test_app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile project/tests/test_app.py\n",
        "import os, pytest\n",
        "from project.app import create_app # Import create_app factory\n",
        "from flask_limiter import Limiter # Still needed for type hinting or if you manually manipulate Limiter\n",
        "from flask_limiter.util import get_remote_address\n",
        "from unittest.mock import patch\n",
        "\n",
        "@pytest.fixture\n",
        "def client(monkeypatch):\n",
        "    # Create a fresh app for each test\n",
        "    test_app = create_app() # Get app\n",
        "    test_app.config['TESTING'] = True\n",
        "    monkeypatch.setenv('FLASK_API_KEY', 'test_api_key')\n",
        "\n",
        "    # Push an application context to ensure everything is set up correctly\n",
        "    with test_app.test_client() as c:\n",
        "        with test_app.app_context(): # Ensure app context for extensions like limiter\n",
        "            yield c\n",
        "    monkeypatch.delenv('FLASK_API_KEY', raising=False)\n",
        "\n",
        "def test_auth_required(client):\n",
        "    r = client.post('/query', json={'input': 'hello'})\n",
        "    assert r.status_code in (401, 403)\n",
        "\n",
        "@patch('project.app.run_agent')\n",
        "def test_query_success(mock_run_agent, client):\n",
        "    mock_run_agent.return_value = {'response': 'ok', 'confidence': 0.9}\n",
        "    headers = {'X-API-Key': 'test_api_key'}\n",
        "    r = client.post('/query', headers=headers, json={'input': 'hello', 'document_content': 'text'})\n",
        "    assert r.status_code == 200\n",
        "    data = r.get_json()\n",
        "    assert 'response' in data and 'privacy' in data\n",
        "    assert data['response']['response'] == 'ok'\n",
        "\n",
        "@patch('project.app.run_agent')\n",
        "def test_rate_limiting(mock_run_agent, client):\n",
        "    mock_run_agent.return_value = {'response': 'ok', 'confidence': 0.9}\n",
        "    headers = {'X-API-Key': 'test_api_key'}\n",
        "    for i in range(10):\n",
        "        r = client.post('/query', headers=headers, json={'input': f'hello {i}'})\n",
        "        assert r.status_code == 200\n",
        "    r = client.post('/query', headers=headers, json={'input': 'exceed'})\n",
        "    assert r.status_code == 429\n",
        "    assert r.get_json()['description'] == 'You have exceeded your rate limit.'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "f6738902",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f6738902",
        "outputId": "cfcb2642-6882-4b09-bbb7-ec9c5a1c0973"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing project/tests/test_e2e.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile project/tests/test_e2e.py\n",
        "import subprocess, sys, os\n",
        "\n",
        "def test_e2e(monkeypatch):\n",
        "    env = os.environ.copy()\n",
        "    env['GOOGLE_API_KEY'] = 'fake_key_for_e2e'\n",
        "    env['E2E_TEST_MODE'] = 'true'\n",
        "    p = subprocess.run([sys.executable, 'project/run_demo.py'], capture_output=True, text=True, env=env)\n",
        "    stdout = p.stdout # Use raw stdout for assertion\n",
        "    # Check for specific debug messages and the mocked response\n",
        "    assert \"E2E_TEST_MODE is ON. Initializing mocked LLM.\" in stdout\n",
        "    assert \"Running agent...\" in stdout\n",
        "    assert \"Mocked LLM response for Hello! This is a demo.\" in stdout\n",
        "    assert \"Agent run finished.\" in stdout\n",
        "    assert p.returncode == 0, f\"Subprocess failed with error: {p.stderr}\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e4f0eae9",
      "metadata": {
        "id": "e4f0eae9"
      },
      "source": [
        "## 10) Run Tests & Coverage (‚â• 90%)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "1bac79aa",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1bac79aa",
        "outputId": "efa67853-1bec-4829-f4ff-72a0216c1305"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Re-running tests and generating coverage report...\n",
            "============================= test session starts ==============================\n",
            "platform linux -- Python 3.12.12, pytest-8.4.2, pluggy-1.6.0\n",
            "rootdir: /content\n",
            "plugins: langsmith-0.4.47, typeguard-4.4.4, anyio-4.11.0\n",
            "collected 39 items\n",
            "\n",
            "project/tests/test_app.py ...                                            [  7%]\n",
            "project/tests/test_e2e.py F                                              [ 10%]\n",
            "project/tests/test_integration.py .                                      [ 12%]\n",
            "project/tests/test_unit.py ..................................            [100%]\n",
            "\n",
            "=================================== FAILURES ===================================\n",
            "___________________________________ test_e2e ___________________________________\n",
            "\n",
            "monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x7dd53d9460c0>\n",
            "\n",
            "    def test_e2e(monkeypatch):\n",
            "        env = os.environ.copy()\n",
            "        env['GOOGLE_API_KEY'] = 'fake_key_for_e2e'\n",
            "        env['E2E_TEST_MODE'] = 'true'\n",
            "        p = subprocess.run([sys.executable, 'project/run_demo.py'], capture_output=True, text=True, env=env)\n",
            "        stdout = p.stdout # Use raw stdout for assertion\n",
            "        # Check for specific debug messages and the mocked response\n",
            ">       assert \"E2E_TEST_MODE is ON. Initializing mocked LLM.\" in stdout\n",
            "E       AssertionError: assert 'E2E_TEST_MODE is ON. Initializing mocked LLM.' in 'DEBUG: Starting run_demo.py script\\nDEBUG: Inside __main__ block.\\nDEBUG: E2E_TEST_MODE is ON. Applying mock.\\n'\n",
            "\n",
            "project/tests/test_e2e.py:10: AssertionError\n",
            "=========================== short test summary info ============================\n",
            "FAILED project/tests/test_e2e.py::test_e2e - AssertionError: assert 'E2E_TEST...\n",
            "======================== 1 failed, 38 passed in 25.79s =========================\n",
            "\n",
            "\n",
            "Name                                  Stmts   Miss  Cover   Missing\n",
            "-------------------------------------------------------------------\n",
            "project/__init__.py                       0      0   100%\n",
            "project/agents/__init__.py                0      0   100%\n",
            "project/agents/evaluator.py              21      1    95%   12\n",
            "project/agents/planner.py                 4      0   100%\n",
            "project/agents/worker.py                 51      2    96%   21-22\n",
            "project/app.py                           31      2    94%   37-38\n",
            "project/core/__init__.py                  0      0   100%\n",
            "project/core/a2a_protocol.py              4      0   100%\n",
            "project/core/context_engineering.py      14      0   100%\n",
            "project/core/observability.py             9      0   100%\n",
            "project/main_agent.py                    30      0   100%\n",
            "project/memory/__init__.py                0      0   100%\n",
            "project/memory/session_memory.py         12      0   100%\n",
            "project/run_demo.py                      34     34     0%   1-39\n",
            "project/tests/__init__.py                 0      0   100%\n",
            "project/tests/test_app.py                36      0   100%\n",
            "project/tests/test_e2e.py                12      4    67%   11-14\n",
            "project/tests/test_integration.py        15      0   100%\n",
            "project/tests/test_unit.py              295      0   100%\n",
            "project/tools/__init__.py                 0      0   100%\n",
            "project/tools/tools.py                  115     10    91%   47-48, 110-114, 139, 141, 143\n",
            "project/ui/__init__.py                    0      0   100%\n",
            "project/ui/i18n.py                        6      1    83%   18\n",
            "-------------------------------------------------------------------\n",
            "TOTAL                                   689     54    92%\n",
            "\n",
            "‚úÖ Test execution and coverage report generated.\n"
          ]
        }
      ],
      "source": [
        "import subprocess\n",
        "\n",
        "print(\"Re-running tests and generating coverage report...\")\n",
        "# Run pytest with coverage\n",
        "pytest_command = ['coverage', 'run', '--rcfile=.coveragerc', '-m', 'pytest', 'project/tests']\n",
        "pytest_process = subprocess.run(pytest_command, capture_output=True, text=True, check=False)\n",
        "print(pytest_process.stdout)\n",
        "print(pytest_process.stderr)\n",
        "\n",
        "# Generate and print the coverage report\n",
        "coverage_report_command = ['coverage', 'report', '--rcfile=.coveragerc', '--show-missing']\n",
        "coverage_report_process = subprocess.run(coverage_report_command, capture_output=True, text=True, check=False)\n",
        "print(coverage_report_process.stdout)\n",
        "print('\\u2705 Test execution and coverage report generated.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8b94a6e0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8b94a6e0",
        "outputId": "37d0b37b-599c-4bde-c4f7-851a3b1b285d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting project/gradio_ui.py\n"
          ]
        }
      ],
      "source": [
        "import gradio as gr\n",
        "from project.main_agent import run_agent\n",
        "from project.tools.tools import extract_pdf_text, extract_docx_text\n",
        "from project.ui.i18n import t\n",
        "import subprocess, os\n",
        "\n",
        "MAX_Q = 15\n",
        "MAX_Q_LEN = 5000\n",
        "MAX_DOC_LEN = 1000000\n",
        "VALID_LANGS = {'auto', 'en', 'es', 'fr', 'nl', 'de'}\n",
        "\n",
        "def read_doc_file(doc_path):\n",
        "    \"\"\"Convert .doc to .txt using LibreOffice headless conversion.\"\"\"\n",
        "    try:\n",
        "        txt_path = doc_path.replace('.doc', '.txt')\n",
        "        subprocess.run(['soffice', '--headless', '--convert-to', 'txt', '--outdir', os.path.dirname(doc_path), doc_path], check=True, timeout=30, capture_output=True)\n",
        "        with open(txt_path, 'r', encoding='utf-8') as f:\n",
        "            return f.read()\n",
        "    except FileNotFoundError:\n",
        "        raise RuntimeError(\"‚ùå LibreOffice not installed. Please install it or upload a .docx/.pdf file instead.\")\n",
        "    except subprocess.TimeoutExpired:\n",
        "        raise RuntimeError(\"‚ùå .doc conversion timed out. Document may be too large.\")\n",
        "    except Exception as e:\n",
        "        raise RuntimeError(f\"‚ùå Failed to convert .doc file: {str(e)}\")\n",
        "\n",
        "def validate_inputs(user_input, doc_content, doc_lang, pref_lang):\n",
        "    \"\"\"Validate user inputs before processing.\"\"\"\n",
        "    errors = []\n",
        "\n",
        "    # Validate question\n",
        "    if not user_input or len(user_input.strip()) == 0:\n",
        "        errors.append(\"‚ùå Question cannot be empty.\")\n",
        "    elif len(user_input) > MAX_Q_LEN:\n",
        "        errors.append(f\"‚ùå Question too long (max {MAX_Q_LEN} chars).\")\n",
        "\n",
        "    # Validate document\n",
        "    if doc_content and len(doc_content) > MAX_DOC_LEN:\n",
        "        errors.append(f\"‚ùå Document too large (max {MAX_DOC_LEN} chars).\")\n",
        "\n",
        "    # Validate language codes\n",
        "    if doc_lang not in VALID_LANGS:\n",
        "        errors.append(f\"‚ùå Invalid document language: {doc_lang}\")\n",
        "    if pref_lang not in {'en', 'es', 'fr', 'nl', 'de'}:\n",
        "        errors.append(f\"‚ùå Invalid reply language: {pref_lang}\")\n",
        "\n",
        "    return errors\n",
        "\n",
        "def process_input(user_input, legal_document, ui_lang, pref_lang, doc_lang, consent_given, counter_state):\n",
        "    print(\"--- Starting process_input function ---\")\n",
        "    if not consent_given:\n",
        "        print(\"--- Consent not given ---\")\n",
        "        return (gr.update(value=f\"### {t('title', ui_lang)}\\n\\n{t('welcome', ui_lang)}\\n\\n**{t('disclaimer', ui_lang)}**\\n\\nPlease agree to the privacy notice to proceed.\"), counter_state)\n",
        "\n",
        "    current = counter_state or 0\n",
        "    if current >= MAX_Q:\n",
        "        print(\"--- Question limit reached ---\")\n",
        "        return (gr.update(value=f\"### {t('title', ui_lang)}\\n\\nLimit reached: You can ask a maximum of {MAX_Q} questions per session.\"), current)\n",
        "\n",
        "    doc_content = None\n",
        "    try:\n",
        "        if legal_document is not None:\n",
        "            name = legal_document.name.lower()\n",
        "            print(f\"--- Processing document: {name} ---\")\n",
        "            if name.endswith('.pdf'):\n",
        "                doc_content = extract_pdf_text(legal_document.name)\n",
        "            elif name.endswith('.docx'):\n",
        "                doc_content = extract_docx_text(legal_document.name)\n",
        "            elif name.endswith('.doc'):\n",
        "                doc_content = read_doc_file(legal_document.name)\n",
        "            else:\n",
        "                with open(legal_document.name, 'r', encoding='utf-8') as f:\n",
        "                    doc_content = f.read()\n",
        "            print(f\"--- Document processed, content length: {len(doc_content) if doc_content else 0} ---\")\n",
        "    except Exception as e:\n",
        "        print(f\"--- Error reading document: {type(e).__name__} - {str(e)} ---\")\n",
        "        return (gr.update(value=f\"### {t('title', ui_lang)}\\n\\n**Error reading document:**\\n\\n**Error Type:** {type(e).__name__}\\n**Message:** {str(e)}\\n\\nPlease try another file.\"), current)\n",
        "\n",
        "    # Validate all inputs\n",
        "    validation_errors = validate_inputs(user_input, doc_content, doc_lang, pref_lang)\n",
        "    if validation_errors:\n",
        "        error_msg = \"\\n\".join(validation_errors)\n",
        "        print(f\"--- Input validation error: {error_msg} ---\")\n",
        "        return (gr.update(value=f\"### {t('title', ui_lang)}\\n\\n**Input Error:**\\n\\n{error_msg}\"), current)\n",
        "\n",
        "    try:\n",
        "        print(\"--- Running main agent ---\")\n",
        "        result = run_agent(user_input, doc_content, doc_lang, pref_lang)\n",
        "        print(\"--- Main agent finished ---\")\n",
        "    except Exception as e:\n",
        "        print(f\"--- Processing Error from main agent: {str(e)} ---\")\n",
        "        return (gr.update(value=f\"### {t('title', ui_lang)}\\n\\n**Processing Error:**\\n\\n{str(e)}\\n\\nPlease try again.\"), current)\n",
        "\n",
        "    current += 1\n",
        "    print(\"--- Returning result from process_input ---\")\n",
        "    return (gr.update(value=f\"### {t('title', ui_lang)}\\n\\n{result['response']}\\n\\n**Confidence:** {result['confidence']}\"), current)\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown('# Expat Legal Aid Advisor')\n",
        "    consent_group = gr.Group(visible=True)\n",
        "    with consent_group:\n",
        "        gr.Markdown('**Privacy Notice**')\n",
        "        gr.Markdown(\"Your input may contain sensitive legal information. We do not persist document contents.\")\n",
        "        consent = gr.Checkbox(label='I agree to the privacy notice', value=False)\n",
        "    main_group = gr.Group(visible=False)\n",
        "    with main_group:\n",
        "        # Input section\n",
        "        gr.Markdown('### üìù Your Input')\n",
        "        user_in = gr.Textbox(label='ü§î Your Legal Question', placeholder='Ask in any language (e.g., English, Spanish, French, etc.)', lines=3)\n",
        "        file_in = gr.File(label='üìÑ Legal Document (Optional)', file_count='single') # Removed file_types constraint\n",
        "\n",
        "        # Language configuration section\n",
        "        gr.Markdown(\"### üåê Language Configuration\")\n",
        "        gr.Markdown(\n",
        "            '**How it works:** The system will translate your document to your chosen communication language, \\n'  # Corrected line\n",
        "            'analyze it, and respond in that language. Auto-detection identifies the document language automatically.'\n",
        "        )\n",
        "\n",
        "        # FIX #2: Define all language dropdowns at same scope level (outside Row) to avoid scope issues\n",
        "        ui_lang = gr.Dropdown(\n",
        "            choices=['en', 'es', 'fr', 'nl', 'de'],\n",
        "            value='en',\n",
        "            label='üé® UI Display Language',\n",
        "            info='Language for interface labels and messages'\n",
        "        )\n",
        "\n",
        "        # Create a row for communication and document language dropdowns\n",
        "        with gr.Row():\n",
        "            pref_lang = gr.Dropdown(\n",
        "                choices=['en', 'es', 'fr', 'nl', 'de'],\n",
        "                value='en',\n",
        "                label='üí¨ Communication Language',\n",
        "                info='Select the language you want to communicate in and receive responses'\n",
        "            )\n",
        "            doc_lang = gr.Dropdown(\n",
        "                choices=['auto', 'en', 'es', 'fr', 'nl', 'de'],\n",
        "                value='auto',\n",
        "                label='üìã Document Language',\n",
        "                info='Choose language or select \"auto\" to auto-detect'\n",
        "            )\n",
        "\n",
        "        # Translation flow info\n",
        "        gr.Markdown(\n",
        "            '**üîÑ Translation Flow:**\\n'\n",
        "            '1. Document language is detected or you specify it\\n'\n",
        "            '2. Document is translated to your communication language\\n'\n",
        "            '3. System analyzes and reasons over the translated content\\n'\n",
        "            '4. Response is generated in your chosen language'\n",
        "        )\n",
        "\n",
        "        gr.Markdown('**‚úÖ Supported Languages:** English, Spanish, French, Dutch, German')\n",
        "\n",
        "        # Submit section\n",
        "        gr.Markdown('### ‚ö° Process')\n",
        "        submit = gr.Button('Submit', variant='primary')\n",
        "        out = gr.Markdown()\n",
        "        counter_state = gr.State(0)\n",
        "\n",
        "    def toggle(consent_val):\n",
        "        return gr.update(visible=not consent_val), gr.update(visible=consent_val)\n",
        "    consent.change(toggle, inputs=[consent], outputs=[consent_group, main_group])\n",
        "    submit.click(fn=process_input, inputs=[user_in, file_in, ui_lang, pref_lang, doc_lang, consent, counter_state], outputs=[out, counter_state])\n",
        "\n",
        "demo.launch(debug=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7170d7c8",
      "metadata": {
        "id": "7170d7c8"
      },
      "source": [
        "## 12) Smoke Tests"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "94aae667",
      "metadata": {
        "id": "94aae667"
      },
      "source": [
        "### 12.1) Agent Smoke (mock LLM to verify Planner‚ÜíWorker‚ÜíEvaluator pipeline)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d8b5d3f1",
      "metadata": {
        "id": "d8b5d3f1"
      },
      "outputs": [],
      "source": [
        "%%writefile project/tests/smoke_test.py\n",
        "import os\n",
        "from unittest.mock import patch\n",
        "from project.main_agent import run_agent\n",
        "\n",
        "# Ensure the directory exists before writing the file\n",
        "os.makedirs('project/tests', exist_ok=True)\n",
        "\n",
        "file_content = \"\"\"\\\n",
        "import os\n",
        "from unittest.mock import patch\n",
        "from project.main_agent import run_agent\n",
        "os.environ['E2E_TEST_MODE'] = 'true'\n",
        "USE_MOCK_FOR_FALLBACK_SMOKE = True\n",
        "if USE_MOCK_FOR_FALLBACK_SMOKE:\n",
        "    with patch('project.tools.tools.GeminiLLM') as MockGeminiLLM:\n",
        "        mock = MockGeminiLLM.return_value\n",
        "        mock.generate_response.return_value = 'Mocked response: Pipeline OK.'\n",
        "        res = run_agent('¬øCu√°les son los requisitos de visa?', 'Este documento menciona visa y permiso.', 'es', 'es')\n",
        "        print('Response:', res['response'][:200])\n",
        "        print('Confidence:', res['confidence'])\n",
        "else:\n",
        "    res = run_agent('What are visa requirements?', 'This document mentions visa and permit.', 'en', 'en')\n",
        "    print('Response:', res['response'][:200])\n",
        "    print('Confidence:', res['confidence'])\n",
        "print('‚úÖ Agent smoke test executed.')\n",
        "\"\"\"\n",
        "\n",
        "with open('project/tests/smoke_test.py', 'w', encoding='utf-8') as f:\n",
        "    f.write(file_content)\n",
        "\n",
        "print('Writing project/tests/smoke_test.py')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "43d89bf5",
      "metadata": {
        "id": "43d89bf5"
      },
      "source": [
        "### 12.2) PDF Parsing Check (generate a PDF and extract its text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "afe98c1e",
      "metadata": {
        "id": "afe98c1e"
      },
      "outputs": [],
      "source": [
        "%%writefile -a project/tests/smoke_test.py\n",
        "import os\n",
        "from reportlab.pdfgen import canvas\n",
        "from project.tools.tools import extract_pdf_text\n",
        "\n",
        "# Ensure the directory exists\n",
        "os.makedirs('project/tests', exist_ok=True)\n",
        "\n",
        "file_content = \"\"\"\\\n",
        "from reportlab.pdfgen import canvas\n",
        "from project.tools.tools import extract_pdf_text\n",
        "pdf_path = 'smoke_test.pdf'\n",
        "c = canvas.Canvas(pdf_path)\n",
        "c.drawString(100, 750, 'This is a smoke test PDF with visa and permit text.')\n",
        "c.save()\n",
        "parsed_text = extract_pdf_text(pdf_path)\n",
        "print('Parsed PDF contains:', 'visa' in parsed_text and 'permit' in parsed_text)\n",
        "print('Parsed snippet:', parsed_text[:120])\n",
        "\"\"\"\n",
        "\n",
        "with open('project/tests/smoke_test.py', 'a', encoding='utf-8') as f:\n",
        "    f.write(file_content)\n",
        "\n",
        "print('Appending to project/tests/smoke_test.py for PDF parsing check.')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c4398359",
      "metadata": {
        "id": "c4398359"
      },
      "source": [
        "### 12.3) DOCX Parsing Check (generate a DOCX and extract its text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1e6b25de",
      "metadata": {
        "id": "1e6b25de"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from docx import Document\n",
        "from project.tools.tools import extract_docx_text\n",
        "\n",
        "# Ensure the directory exists\n",
        "os.makedirs('project/tests', exist_ok=True)\n",
        "\n",
        "file_content = \"\"\"\\\n",
        "from docx import Document\n",
        "from project.tools.tools import extract_docx_text\n",
        "\n",
        "path = 'smoke_test.docx'\n",
        "doc = Document()\n",
        "doc.add_paragraph('This is a smoke test DOCX with residence permit text.')\n",
        "doc.save(path)\n",
        "parsed_docx = extract_docx_text(path)\n",
        "print('Parsed DOCX contains:', 'residence permit' in parsed_docx)\n",
        "print('Parsed snippet:', parsed_docx[:120])\n",
        "\"\"\"\n",
        "\n",
        "with open('project/tests/smoke_test.py', 'a', encoding='utf-8') as f:\n",
        "    f.write(file_content)\n",
        "\n",
        "print('Appending to project/tests/smoke_test.py for DOCX parsing check.')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b06b21ef",
      "metadata": {
        "id": "b06b21ef"
      },
      "source": [
        "### 12.4) Translation Failure Fallback (force translator to fail; LLM uses original doc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3e9bfde1",
      "metadata": {
        "id": "3e9bfde1"
      },
      "outputs": [],
      "source": [
        "%%writefile -a project/tests/smoke_test.py\n",
        "import os\n",
        "from unittest.mock import patch\n",
        "from project.main_agent import run_agent\n",
        "\n",
        "# Ensure the directory exists\n",
        "os.makedirs('project/tests', exist_ok=True)\n",
        "\n",
        "file_content = \"\"\"\\\n",
        "from unittest.mock import patch\n",
        "from project.main_agent import run_agent\n",
        "\n",
        "# Force translator failure\n",
        "with patch('project.tools.tools.GoogleTranslator.translate', side_effect=Exception('Forced failure')):\n",
        "    res = run_agent('Pregunta en espa√±ol sobre residencia', 'Documento original en espa√±ol con detalles de visa y permiso de trabajo.')\n",
        "    print('Response (fallback path):', res['response'][:200])\n",
        "    print('Confidence:', res['confidence'])\n",
        "print('‚úÖ Fallback smoke test executed.')\n",
        "\"\"\"\n",
        "\n",
        "with open('project/tests/smoke_test.py', 'a', encoding='utf-8') as f:\n",
        "    f.write(file_content)\n",
        "\n",
        "print('Appending to project/tests/smoke_test.py for Translation Fallback check.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f4561201",
      "metadata": {
        "id": "f4561201"
      },
      "outputs": [],
      "source": [
        "%%writefile -a project/tests/smoke_test.py\n",
        "from reportlab.pdfgen import canvas\n",
        "from project.tools.tools import extract_pdf_text\n",
        "pdf_path = 'smoke_test.pdf'\n",
        "c = canvas.Canvas(str(pdf_path)) # Convert PosixPath to string\n",
        "c.drawString(100, 750, 'This is a smoke test PDF with visa and permit text.')\n",
        "c.save()\n",
        "parsed_text = extract_pdf_text(pdf_path)\n",
        "print('Parsed PDF contains:', 'visa' in parsed_text and 'permit' in parsed_text)\n",
        "print('Parsed snippet:', parsed_text[:120])\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
